from Tokenizer import Tokenizer

tok = Tokenizer()

# tok.filter_tokens_by_vocab_size()
# tok.set_start_and_end_tokens()
# tok.save_tokenizer()

tok.print_tokenizer()
